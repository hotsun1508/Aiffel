{"cells":[{"cell_type":"markdown","metadata":{"id":"PZytpay1IgXD"},"source":["# FUNDAMENTALS 21. TF2 API 개요\n","\n","##🍎 학습목표\n","- Tensorflow V2의 개요와 특징을 파악한다.\n","- Tensorflow V2의 3가지 주요 API 구성 방식을 이해하고 활용할 수 있다.\n","- GradientTape를 활용해 보고 좀 더 로우 레벨의 딥러닝 구현 방식을 이해한다.\n","\n","### 🍎 TensorFlow2에서 딥러닝 모델을 작성하는 방법 3가지\n","- Sequential, Functional, 그리고 Model Subclassing입니다. \n","- 아마 Sequential 모델은 이미 몇 번 사용해 보셔서 정확한 개념은 몰라도 익숙하실 것입니다. Functional은 Sequential의 보다 일반화된 개념입니다.\n","- Subclassing은 클래스로 구현된 기존의 모델을 상속받아 자신만의 모델을 만들어나가는 방식입니다.\n"]},{"cell_type":"markdown","metadata":{"id":"X7UcPFKtKJxt"},"source":["## 1) TensorFlow2 Sequential Model\n","```\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","model = keras.Sequential()\n","model.add(__넣고싶은 레이어__)\n","model.add(__넣고싶은 레이어__)\n","model.add(__넣고싶은 레이어__)\n","\n","model.fit(x, y, epochs=10, batch_size=32)\n","```\n","앞선 자료에서 공부했던 모델은 대부분 위와 같은 형식이었습니다. `model = keras.Sequential()` 이라고 선언한 부분이 눈에 띄시나요? Sequential 모델을 활용하면 손쉽게 딥러닝 모델을 쌓아나갈 수 있습니다. 입력부터 출력까지 레이어를 그야말로 sequential하게 차곡차곡 add해서 쌓아나가기만 하면 됩니다. 무엇보다 이 방식은 초보자가 접근하기에 매우 쉽다는 장점이 있습니다. 그렇지만 모델의 입력과 출력이 여러 개인 경우에는 적합하지 않은 모델링 방식입니다. Sequential 모델은 반드시 입력 1가지, 출력 1가지를 전제로 합니다.\n","\n","아래 참고 자료에서 Sequential Model로 작성한 모델의 전체 코드를 확인해 주세요. 실습 세션에서 해당 모델링을 직접 진행해 볼 예정입니다.\n","\n","- [텐서플로 2.0 시작하기: 초보자용](https://www.tensorflow.org/tutorials/quickstart/beginner)\n","\n","## 2) TensorFlow2 Functional API\n","```\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","inputs = keras.Input(shape=(__원하는 입력값 모양__))\n","x = keras.layers.__넣고싶은 레이어__(관련 파라미터)(input)\n","x = keras.layers.__넣고싶은 레이어__(관련 파라미터)(x)\n","outputs = keras.layers.__넣고싶은 레이어__(관련 파라미터)(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","model.fit(x,y, epochs=10, batch_size=32)\n","```\n","여기서 위 `Sequential Model`을 활용하는 것과 다른 점은 바로 `keras.Model`을 사용한다는 점입니다. 그래서 `Sequential Model`을 쓰는 것보다 더 일반적인 접근인 것입니다. `Sequential Model`이란 사실 `keras.Model`을 상속받아 확장한 특수 사례에 불과한 것이니까요. `Functional API`를 활용하면 앞서 배운 `Sequential Model`을 활용하는 것보다 더 자유로운 모델링을 진행할 수 있습니다. `Functional`이라는 뜻이 뭔가요? 함수형으로 모델을 구성한다는 것, 즉 **입력과 출력을 규정함으로써 모델 전체를 규정**한다는 생각입니다. 그래서 이번에는 `Input`이라는 것을 규정합니다. `Input`이 될 수 있는 텐서가 여러 개가 될 수도 있습니다. 그리고 레이어들을 자유롭게 엮어 출력(Output)까지 규정하면 Model이란 바로 `inputs`와 `outputs` 만으로 규정됩니다. 정말 Functional하지 않나요?\n","\n","Sequential Model의 제약점이 1개의 입력/출력이었다면 Functional API를 통해 다중 입력/출력을 가지는 모델을 구성할 수 있습니다.\n","\n","아래 참고 자료에서 Functional API를 활용해서 모델링 하는 방법에 대해서 살펴봐 주세요. 실습 세션에서 해당 모델링을 직접 진행해 볼 예정입니다.\n","\n","- [The Keras functional API](https://www.tensorflow.org/guide/keras/functional)\n","\n","## 3) TensorFlow2 Subclassing\n","```\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","class CustomModel(keras.Model):\n","    def __init__(self):\n","        super(CustomModel, self).__init__()\n","        self.__정의하고자 하는 레이어__()\n","        self.__정의하고자 하는 레이어__()\n","        self.__정의하고자 하는 레이어__()\n","    \n","    def call(self, x):\n","        x = self.__정의하고자 하는 레이어__(x)\n","        x = self.__정의하고자 하는 레이어__(x)\n","        x = self.__정의하고자 하는 레이어__(x)\n","        \n","        return x\n","    \n","model = CustomModel()\n","model.fit(x,y, epochs=10, batch_size=32)\n","```\n","마지막으로 `Subclassing`을 활용하면 제일 자유로운 모델링을 진행할 수 있습니다. 사실 본질적으로는 `Functional`한 접근과 차이가 없습니다. 이것은 `keras.Model`을 상속받은 모델 클래스를 만드는 것이기 때문입니다. 처음 만났던 `Sequential Model`도 따지고 보면 `keras.Model`을 상속받은 모델 클래스의 하나일 뿐입니다. `keras.Model`은 위와 같이 `__init__()`이라는 메서드 안에서 레이어 구성을 정의합니다. 그리고 `call()`이라는 메서드 안에서 레이어 간 `forward propagation`을 구현합니다. 이것으로 끝입니다. 다만, 각 레이어에 대한 깊은 이해가 필요하고 초심자에게 의도치 않은 버그를 유발할 수 있습니다. 그렇지만 여러분들이 공부해 나가시면서 복잡한 모델링을 진행하시게 되면 가장 많이 접하게 되실 모델링 스타일이기에 실습 세션에서 다뤄볼 예정입니다.\n","\n","아래 참고 자료에서 `Subclassing`을 활용한 모델링에 대해서 살펴봐주세요. 실습세션에서 해당 모델링을 직접 진행해볼 예정입니다.\n","\n","- [텐서플로 2.0 시작하기: 전문가용](https://www.tensorflow.org/tutorials/quickstart/advanced)"]},{"cell_type":"markdown","metadata":{"id":"SZEFaVlFPFCn"},"source":["## Tensorflow2 API로 모델 작성하기\n","### 🍎 MNIST (1) Sequential API 활용"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8341,"status":"ok","timestamp":1649691473909,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"nJ-MKjbGGDwD"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1522,"status":"ok","timestamp":1649691475424,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"ov4swdN0HaQc","outputId":"42bd9283-b15f-401a-9e3d-677a7760e89a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","60000 10000\n"]}],"source":["# 데이터 구성부분\n","mnist = keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train=x_train[...,np.newaxis]  # : = ... 같은거임 !!!\n","x_test=x_test[...,np.newaxis]\n","\n","print(len(x_train), len(x_test))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":561,"status":"ok","timestamp":1649691475982,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"LgQrFbTvHaLO"},"outputs":[],"source":["# Sequential Model을 구성해주세요.\n","\"\"\"\n","Spec:\n","1. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","2. 64개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","3. Flatten 레이어\n","4. 128개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n","5. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n","\"\"\"\n","\n","# 여기에 모델을 구성해주세요\n","model = keras.Sequential([\n","    keras.layers.Conv2D(32, 3, activation='relu'),\n","    keras.layers.Conv2D(64, 3, activation='relu'),\n","    keras.layers.Flatten(),\n","\tkeras.layers.Dense(128, activation='relu'),\n","    keras.layers.Dense(10, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1175017,"status":"ok","timestamp":1649692650996,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"UwfEa2E1HaJG","outputId":"f345ee3c-460d-47e7-cf2f-0a0094c2530f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1875/1875 [==============================] - 238s 125ms/step - loss: 0.1066 - accuracy: 0.9674\n","Epoch 2/5\n","1875/1875 [==============================] - 230s 123ms/step - loss: 0.0354 - accuracy: 0.9886\n","Epoch 3/5\n","1875/1875 [==============================] - 231s 123ms/step - loss: 0.0194 - accuracy: 0.9937\n","Epoch 4/5\n","1875/1875 [==============================] - 233s 124ms/step - loss: 0.0140 - accuracy: 0.9955\n","Epoch 5/5\n","1875/1875 [==============================] - 232s 123ms/step - loss: 0.0096 - accuracy: 0.9967\n","313/313 - 8s - loss: 0.0528 - accuracy: 0.9885 - 8s/epoch - 25ms/step\n"]},{"data":{"text/plain":["[0.05278196558356285, 0.9884999990463257]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 학습 설정\n","\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","\n","model.evaluate(x_test,  y_test, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"W3Ptk5rERgVK"},"source":["###🍎 MNIST (2) Functional API 활용\n","- `keras.Model`을 직접 활용\n","- `keras.Input`으로 정의된 input 및 output 레이어 구성을 통해 model 구현"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1649692650997,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"EuG7bNzXHaGc"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1649692651525,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"k1Z9AFuFHaDm","outputId":"90f29a21-926e-449d-eaea-3fef8a1bb47c"},"outputs":[{"name":"stdout","output_type":"stream","text":["60000 10000\n"]}],"source":["mnist = keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train=x_train[...,np.newaxis]\n","x_test=x_test[...,np.newaxis]\n","\n","print(len(x_train), len(x_test))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649692651526,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"ervFOL-fRzjl"},"outputs":[],"source":["\"\"\"\n","Spec:\n","0. (28X28X1) 차원으로 정의된 Input\n","1. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","2. 64개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","3. Flatten 레이어\n","4. 128개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n","5. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n","\"\"\"\n","\n","# 여기에 모델을 구성해 주세요.\n","inputs = keras.Input(shape=(28, 28, 1))\n","\n","x = keras.layers.Conv2D(32, 3, activation='relu')(inputs)\n","x = keras.layers.Conv2D(64, 3, activation='relu')(x)\n","x = keras.layers.Flatten()(x)\n","x = keras.layers.Dense(128, activation='relu')(x)\n","predictions = keras.layers.Dense(10, activation='softmax')(x)\n","\n","model = keras.Model(inputs=inputs, outputs=predictions)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1170832,"status":"ok","timestamp":1649693822351,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"H9Bc57-hHaA3","outputId":"4abb5034-a7c9-4fac-ea59-f3df3b069d15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1875/1875 [==============================] - 230s 122ms/step - loss: 0.1040 - accuracy: 0.9678\n","Epoch 2/5\n","1875/1875 [==============================] - 231s 123ms/step - loss: 0.0342 - accuracy: 0.9894\n","Epoch 3/5\n","1875/1875 [==============================] - 234s 125ms/step - loss: 0.0211 - accuracy: 0.9927\n","Epoch 4/5\n","1875/1875 [==============================] - 228s 122ms/step - loss: 0.0128 - accuracy: 0.9960\n","Epoch 5/5\n","1875/1875 [==============================] - 231s 123ms/step - loss: 0.0085 - accuracy: 0.9972\n","313/313 - 8s - loss: 0.0551 - accuracy: 0.9862 - 8s/epoch - 26ms/step\n"]},{"data":{"text/plain":["[0.05506592243909836, 0.9861999750137329]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 학습 설정\n","\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","\n","model.evaluate(x_test,  y_test, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"u-b5ZbpHSs8U"},"source":["##🍎 MNIST (3) Subclassing 활용\n","- `Subclassing` 방법은 `keras.Model`을 상속받은 클래스를 만드는 것\n","- `__init__()` 메서드 안에서 레이어를 선언\n","- `call()` 메서드 안에서 `forward propagation`을 구현하는 방식\n","- `Functional` 방식과 비교하자면, `call()`의 입력이 Input이고, `call()`의 리턴값이 Output이 된다"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1649693822352,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"-1QMc3GmHZ-b"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":552,"status":"ok","timestamp":1649693822893,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"_VjaP4iwHZ8B","outputId":"840948d5-6eb7-413a-ad95-91fc47dce17b"},"outputs":[{"name":"stdout","output_type":"stream","text":["60000 10000\n"]}],"source":["# 데이터 구성부분\n","mnist = keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","x_train=x_train[...,np.newaxis]\n","x_test=x_test[...,np.newaxis]\n","\n","print(len(x_train), len(x_test))"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649693822894,"user":{"displayName":"Sun Ah Min","userId":"07602283813262024316"},"user_tz":-540},"id":"UPl88cskTGfk"},"outputs":[],"source":["# Subclassing을 활용한 Model을 구성해주세요.\n","\"\"\"\n","Spec:\n","0. keras.Model 을 상속받았으며, __init__()와 call() 메서드를 가진 모델 클래스\n","1. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","2. 64개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","3. Flatten 레이어\n","4. 128개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n","5. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n","6. call의 입력값이 모델의 Input, call의 리턴값이 모델의 Output\n","\"\"\"\n","\n","# 여기에 모델을 구성해주세요\n","class CustomModel(keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = keras.layers.Conv2D(32, 3, activation='relu')\n","        self.conv2 = keras.layers.Conv2D(64, 3, activation='relu')\n","        self.flatten = keras.layers.Flatten()\n","        self.fc1 = keras.layers.Dense(128, activation='relu')\n","        self.fc2 = keras.layers.Dense(10, activation='softmax')\n","        \n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        \n","        return x\n","        \n","model = CustomModel()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"pfPdOTvIHZ5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1875/1875 [==============================] - 228s 121ms/step - loss: 0.1075 - accuracy: 0.9669\n","Epoch 2/5\n","1875/1875 [==============================] - 224s 120ms/step - loss: 0.0340 - accuracy: 0.9892\n","Epoch 3/5\n","1875/1875 [==============================] - 228s 121ms/step - loss: 0.0201 - accuracy: 0.9936\n","Epoch 4/5\n","1875/1875 [==============================] - 239s 127ms/step - loss: 0.0126 - accuracy: 0.9959\n","Epoch 5/5\n","1875/1875 [==============================] - 237s 126ms/step - loss: 0.0101 - accuracy: 0.9966\n","313/313 - 8s - loss: 0.0399 - accuracy: 0.9882 - 8s/epoch - 25ms/step\n"]},{"data":{"text/plain":["[0.039857883006334305, 0.9882000088691711]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 학습 설정\n","\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","\n","model.evaluate(x_test,  y_test, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"bn2OmxXFTwwM"},"source":["## TensorFlow2 API로 모델 작성 및 학습하기\n","###🍎 CIFAR-100 (1) Sequential API 활용\n","\u003e #### [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)  \n","The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XS5O4X9nHZ3D"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AELC7JX-HZ0O"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n","169009152/169001437 [==============================] - 11s 0us/step\n","169017344/169001437 [==============================] - 11s 0us/step\n","50000 10000\n"]}],"source":["# 데이터 구성부분\n","cifar100 = keras.datasets.cifar100\n","\n","(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","print(len(x_train), len(x_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xkj7MqC_HZve"},"outputs":[],"source":["# Sequential Model을 구성해주세요.\n","\"\"\"\n","Spec:\n","1. 16개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","2. pool_size가 2인 MaxPool 레이어\n","3. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","4. pool_size가 2인 MaxPool 레이어\n","5. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n","6. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n","\"\"\"\n","\n","# 여기에 모델을 구성해주세요\n","model = keras.Sequential([\n","    keras.layers.Conv2D(16, 3, activation='relu'),\n","    keras.layers.MaxPool2D((2,2)),\n","    keras.layers.Conv2D(32, 3, activation='relu'),\n","    keras.layers.MaxPool2D((2,2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(256, activation='relu'),\n","    keras.layers.Dense(100, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yzDhK8ouUMZ0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1563/1563 [==============================] - 47s 30ms/step - loss: 3.6144 - accuracy: 0.1575\n","Epoch 2/5\n","1563/1563 [==============================] - 47s 30ms/step - loss: 2.9316 - accuracy: 0.2760\n","Epoch 3/5\n","1563/1563 [==============================] - 46s 30ms/step - loss: 2.6318 - accuracy: 0.3367\n","Epoch 4/5\n","1563/1563 [==============================] - 46s 29ms/step - loss: 2.4299 - accuracy: 0.3785\n","Epoch 5/5\n","1563/1563 [==============================] - 46s 29ms/step - loss: 2.2667 - accuracy: 0.4149\n","313/313 - 3s - loss: 2.6136 - accuracy: 0.3501 - 3s/epoch - 10ms/step\n"]},{"data":{"text/plain":["[2.6135876178741455, 0.35010001063346863]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 학습 설정\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# 학습 관련 부분을 작성해주세요\n","model.fit(x_train, y_train, epochs=5)\n","\n","model.evaluate(x_test,  y_test, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"8HY2ntBOXpXJ"},"source":["##🍎 CIFAR-100 (2) Functional API 활용\n","- `keras.Model`을 직접 활용\n","- `keras.Input`으로 정의된 input 및 output 레이어 구성을 통해 model 구현"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BYpsyD4bUOU0"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"a01ZrPpZHZs7"},"outputs":[{"name":"stdout","output_type":"stream","text":["50000 10000\n"]}],"source":["cifar100 = keras.datasets.cifar100\n","\n","(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","print(len(x_train), len(x_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kj-30AeBHZrY"},"outputs":[],"source":["# Functional API를 활용한 Model을 구성해주세요.\n","\"\"\"\n","Spec:\n","0. (32X32X3) 차원으로 정의된 Input\n","1. 16개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","2. pool_size가 2인 MaxPool 레이어\n","3. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","4. pool_size가 2인 MaxPool 레이어\n","5. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n","6. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n","\"\"\"\n","\n","# 여기에 모델을 구성해주세요\n","inputs = keras.Input(shape=(32, 32, 3))\n","\n","x = keras.layers.Conv2D(16, 3, activation='relu')(inputs)\n","x = keras.layers.MaxPool2D((2,2))(x)\n","x = keras.layers.Conv2D(32, 3, activation='relu')(x)\n","x = keras.layers.MaxPool2D((2,2))(x)\n","x = keras.layers.Flatten()(x)\n","x = keras.layers.Dense(256, activation='relu')(x)\n","predictions = keras.layers.Dense(100, activation='softmax')(x)\n","\n","model = keras.Model(inputs=inputs, outputs=predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ds55lkD3YcYL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1563/1563 [==============================] - 49s 31ms/step - loss: 3.6466 - accuracy: 0.1524\n","Epoch 2/5\n","1563/1563 [==============================] - 45s 29ms/step - loss: 2.9169 - accuracy: 0.2829\n","Epoch 3/5\n","1563/1563 [==============================] - 47s 30ms/step - loss: 2.6171 - accuracy: 0.3404\n","Epoch 4/5\n","1563/1563 [==============================] - 47s 30ms/step - loss: 2.4158 - accuracy: 0.3838\n","Epoch 5/5\n","1563/1563 [==============================] - 46s 30ms/step - loss: 2.2494 - accuracy: 0.4201\n","313/313 - 3s - loss: 2.5922 - accuracy: 0.3537 - 3s/epoch - 10ms/step\n"]},{"data":{"text/plain":["[2.5922296047210693, 0.35370001196861267]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 학습 설정\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# 학습 관련 부분을 작성해주세요\n","model.fit(x_train, y_train, epochs=5)\n","\n","model.evaluate(x_test,  y_test, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"wX8SkxyBYkZn"},"source":["## 🍎 CIFAR-100 (3) Subclassing 활용\n","- `Subclassing` 방법은 `keras.Model`을 상속받은 클래스를 만드는 것\n","- `__init__()` 메서드 안에서 레이어를 선언\n","- `call()` 메서드 안에서 `forward propagation`을 구현하는 방식\n","- `Functional` 방식과 비교하자면, `call()`의 입력이 Input이고, `call()`의 리턴값이 Output이 된다\n","\n","여전히 정확도는 40% 미만의 수치"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vEWP1HlZYeLv"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4MuETyOlHZnS"},"outputs":[{"name":"stdout","output_type":"stream","text":["50000 10000\n"]}],"source":["# 데이터 구성부분\n","cifar100 = keras.datasets.cifar100\n","\n","(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","print(len(x_train), len(x_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ES66_hHlHZkb"},"outputs":[],"source":["# Subclassing을 활용한 Model을 구성해주세요.\n","\"\"\"\n","Spec:\n","0. keras.Model 을 상속받았으며, __init__()와 call() 메서드를 가진 모델 클래스\n","1. 16개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","2. pool_size가 2인 MaxPool 레이어\n","3. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n","4. pool_size가 2인 MaxPool 레이어\n","5. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n","6. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n","7. call의 입력값이 모델의 Input, call의 리턴값이 모델의 Output\n","\"\"\"\n","\n","# 여기에 모델을 구성해주세요\n","class CustomModel(keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = keras.layers.Conv2D(16, 3, activation='relu')\n","        self.maxpool1 = keras.layers.MaxPool2D((2,2))\n","        self.conv2 = keras.layers.Conv2D(32, 3, activation='relu')\n","        self.maxpool2 = keras.layers.MaxPool2D((2,2))\n","        self.flatten = keras.layers.Flatten()\n","        self.fc1 = keras.layers.Dense(256, activation='relu')\n","        self.fc2 = keras.layers.Dense(100, activation='softmax')\n","        \n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool1(x)\n","        x = self.conv2(x)\n","        x = self.maxpool2(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        \n","        return x\n","        \n","model = CustomModel()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ofu01rr3HZhv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1563/1563 [==============================] - 46s 29ms/step - loss: 3.7016 - accuracy: 0.1415\n","Epoch 2/5\n","1563/1563 [==============================] - 46s 29ms/step - loss: 3.0244 - accuracy: 0.2579\n","Epoch 3/5\n","1563/1563 [==============================] - 47s 30ms/step - loss: 2.7375 - accuracy: 0.3152\n","Epoch 4/5\n","1563/1563 [==============================] - 46s 29ms/step - loss: 2.5426 - accuracy: 0.3569\n","Epoch 5/5\n","1563/1563 [==============================] - 46s 30ms/step - loss: 2.3884 - accuracy: 0.3883\n","313/313 - 3s - loss: 2.6665 - accuracy: 0.3469 - 3s/epoch - 9ms/step\n"]},{"data":{"text/plain":["[2.6665449142456055, 0.34689998626708984]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 학습 설정\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# 학습 관련 부분을 작성해주세요\n","model.fit(x_train, y_train, epochs=5)\n","\n","model.evaluate(x_test,  y_test, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"RuS8op1lY4LC"},"source":["## 🍎 GradientTape의 활용\n","### Automatic differentiation - GradientTape\n","우리는 조금 전까지 아주 비슷한 테스크 2개를, 본질적으로 큰 차이가 없는 3개의 모델 구성 방법을 활용하여 딥러닝으로 구현해 보았습니다. 그동안 완전히 동일하게 구성했던 것은 바로 아래와 같이 구성된 모델 학습 관련 부분입니다.\n","```\n","# 모델 학습 설정\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","```\n","Numpy만 가지고 딥러닝을 구현하는 것을 회상해 봅시다. `model.fit()`이라는 한 줄로 수행 가능한 딥러닝 모델 훈련 과정은 실제로는 어떠했나요?\n","###🍎 train_step\n","1. Forward Propagation 수행 및 중간 레이어값 저장\n","2. Loss 값 계산\n","3. 중간 레이어값 및 Loss를 활용한 체인룰(chain rule) 방식의 역전파(Backward Propagation) 수행\n","4. 학습 파라미터 업데이트\n","\u003e 이런 과정이 TF2 API에는 `model.fit()`이라는 메서드 안에 모두 추상화되어 포함되어 있다.\n","\n","##🍎 Tensorflow에서 제공하는 `tf.GradientTape`\n","- 위와 같이 순전파(forward pass) 로 진행된 모든 연산의 중간 레이어값을 **tape**에 기록하고, 이를 이용해 gradient를 계산한 후 **tape**를 폐기하는 기능을 수행\n","- 그래디언트를 좀 더 고급스럽게 활용하는 다양한 기법을 통해 자주 사용됨\n","\n","### 이전 스텝에서 진행했던 학습을 `tf.GradientTape`를 이용한 것으로 변형해 보자. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2C8mnpjsHZe8"},"outputs":[{"name":"stdout","output_type":"stream","text":["50000 10000\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","# 데이터 구성부분\n","cifar100 = keras.datasets.cifar100\n","\n","(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","print(len(x_train), len(x_test))\n","\n","# 모델 구성부분\n","class CustomModel(keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = keras.layers.Conv2D(16, 3, activation='relu')\n","        self.maxpool1 = keras.layers.MaxPool2D((2,2))\n","        self.conv2 = keras.layers.Conv2D(32, 3, activation='relu')\n","        self.maxpool2 = keras.layers.MaxPool2D((2,2))\n","        self.flatten = keras.layers.Flatten()\n","        self.fc1 = keras.layers.Dense(256, activation='relu')\n","        self.fc2 = keras.layers.Dense(100, activation='softmax')\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool1(x)\n","        x = self.conv2(x)\n","        x = self.maxpool2(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","model = CustomModel()"]},{"cell_type":"markdown","metadata":{"id":"wq9yguFHaGcW"},"source":["여기까지는 앞에서 다루었던 Subclassing을 활용한 모델 작성법과 전혀 다르지 않습니다. 달라지는 것은 model.compile(), model.fit()을 통해 손쉽게 진행했던 학습 세팅 및 수행 부분입니다.\n","```\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","```\n","- **model.compile()**: 모델 학습을 위해 `loss`, `optimizer`를 지정해 주면 내부적으로는 매 스텝 학습이 진행될 때마다 발생하는 loss 및 그래디언트가 어떻게 학습 파라미터를 업데이트하게 되는지를 지정해 주는 작업이 `model.compile()` 안에서 자동으로 진행\n","\n","### 🍎 아래 코드\n","- `tape.gradient()`를 통해 매 스텝 학습이 진행될 때마다 발생하는 그래디언트를 추출한 후 \n","- `optimizer.apply_gradients()`를 통해 발생한 그래디언트가 \n","- 업데이트해야 할 파라미터 `model.trainable_variables`를 지정해 주는 과정을 기술한 것입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SvnXlNSRHZcM"},"outputs":[],"source":["loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# tf.GradientTape()를 활용한 train_step\n","def train_step(features, labels):\n","    with tf.GradientTape() as tape:  # with로 GradientTape()를 tape로 잠깐 사용 후 반납 \n","        predictions = model(features)\n","        loss = loss_func(labels, predictions)\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # zip 압축\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"nRIrsmQ1aKWq"},"source":["\u003e 매 스텝 진행되는 학습의 실제 동작이 `train_step()` 메서드로 구현되었습니다.\n","```\n","model.fit(x_train, y_train, epochs=5, batch_size=32)\n","```\n","### `model.fit()`으로 위와 같이 한 줄로 간단하게 수행되던 실제 배치 학습 과정\n","- 매 스텝마다 위에서 구현했던 `train_step()`가 호출되는 과정으로 바꾸어 구현 가능. \n","- `model.fit()` 호출 시에 결정되는 `batch_size`만 결정해 주면 된다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ouJdPFjwHZZb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: last batch loss = 3.0676\n","Epoch 1: last batch loss = 2.5280\n","Epoch 2: last batch loss = 2.3113\n","Epoch 3: last batch loss = 2.1450\n","Epoch 4: last batch loss = 2.0090\n","It took 374.85196232795715 seconds\n"]}],"source":["import time\n","def train_model(batch_size=32):\n","    start = time.time()\n","    for epoch in range(5):\n","        x_batch = []\n","        y_batch = []\n","        for step, (x, y) in enumerate(zip(x_train, y_train)):  # x_train, y_train 쌍의 크기만큼의 step마다 (x_train이랑 y_train)이 쌍으로 append\n","            x_batch.append(x)\n","            y_batch.append(y)\n","            if step % batch_size == batch_size-1:  # 이걸 만족하는 경우 x_batch, y_batch 초기화 // step =0부터 시작 batch size 0~31까지 가야지만 batch_size=32가 되니까..ㅇㅇ\n","                loss = train_step(np.array(x_batch, dtype=np.float32), np.array(y_batch, dtype=np.float32)) # epoch별로 loss값을 계산하기 위해 필요한 코드\n","                x_batch = []\n","                y_batch = []\n","        print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss))) \n","    print(\"It took {} seconds\".format(time.time() - start))\n","\n","train_model()"]},{"cell_type":"markdown","metadata":{"id":"kY61OTHiaNli"},"source":["\u003e #### 🍎 위에서 구현한 `train_model()` 메서드가 실은 우리가 그동안 사용했던 `model.fit()` 메서드와 기능적으로 같다.\n","\n","- 이렇듯 `tf.GradientTape()`를 활용하면 `model.compile()`과 `model.fit()` 안에 감추어져 있던 한 스텝의 학습 단계(위 예제에서는 `train_step` 메서드)를 끄집어내서 자유롭게 재구성\n","- 지도학습 방식과 다른 강화학습 또는 GAN(Generative Advasarial Network)의 학습을 위해서는 `train_step` 메서드의 재구성이 필수적이므로 `tf.GradientTape()`의 활용법 숙지 필요 !!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"H3UtC9ZqHZUM"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 2s 2s/step\n"]},{"data":{"text/plain":["0.3441"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# evaluation\n","prediction = model.predict(x_test, batch_size=x_test.shape[0], verbose=1)\n","temp = sum(np.squeeze(y_test) == np.argmax(prediction, axis=1))  # squeeze: 3*2*1인 경우 1차원의 배열 삭제 // 확률값 가장 높은 것 argmax: prediction값들 중 max index -\u003e 만족하는게 True인 값들만 temp\n","temp/len(y_test)  # Accuracy  # 실제 라벨하고 prediction의 max값이 가장 큰 거랑 같은 애들만 뽑아옴. "]},{"cell_type":"markdown","metadata":{"id":"UEAvJNmLaSJI"},"source":["- 그래디언트를 활용할 필요가 없는 evaluation 단계: 기존 `model.predict()` 메서드 다시 활용\n","- 충분한 성능을 확인할 수 있을 만큼의 학습이 진행된 상태가 아니니 최종 Accuracy 값은 신경 ❌\n","\n","여기까지 Tensorflow V2 API 활용법을 살펴보았습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qV1Iu4IgHZLC"},"outputs":[{"data":{"text/plain":["array([[49],\n","       [33],\n","       [72],\n","       ...,\n","       [51],\n","       [42],\n","       [70]])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["y_test"]},{"cell_type":"markdown","metadata":{"id":"xhCWq1YhRd1g"},"source":[""]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOomJcyK4OigepMNvqs7c24","collapsed_sections":[],"name":"[F-21]Tensorflow2_API.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}