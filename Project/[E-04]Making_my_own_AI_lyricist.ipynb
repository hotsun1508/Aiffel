{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe781ed",
   "metadata": {},
   "source": [
    "# Exploration 4 - ë©‹ì§„ ì¸ê³µì§€ëŠ¥ ì‘ì‚¬ê°€ ë§Œë“¤ê¸° ğŸµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0c32d",
   "metadata": {},
   "source": [
    "### Step 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dff543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951b980",
   "metadata": {},
   "source": [
    "### Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
    "- glob ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, raw_corpus ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c599c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()  # í–‰(ì¤„) ë‹¨ìœ„ë¡œ ë‹´ê¸°\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf2007",
   "metadata": {},
   "source": [
    "### Step 3. ë°ì´í„° ì •ì œí•˜ê¸°\n",
    "<!-- \n",
    "# ì…ë ¥ëœ ë¬¸ì¥ì„\n",
    "#     1. ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³ , ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤\n",
    "#     2. íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ë„£ê³ \n",
    "#     3. ì—¬ëŸ¬ê°œì˜ ê³µë°±ì€ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤\n",
    "#     4. a-zA-Z?.!,Â¿ê°€ ì•„ë‹Œ ëª¨ë“  ë¬¸ìë¥¼ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤\n",
    "#     5. ë‹¤ì‹œ ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤\n",
    "#     6. ë¬¸ì¥ ì‹œì‘ì—ëŠ” <start>, ëì—ëŠ” <end>ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤\n",
    "# ì´ ìˆœì„œë¡œ ì²˜ë¦¬í•´ì£¼ë©´ ë¬¸ì œê°€ ë˜ëŠ” ìƒí™©ì„ ë°©ì§€í•  ìˆ˜ ìˆê² ë„¤ìš”! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e16a1",
   "metadata": {},
   "source": [
    "í† í°í™”í•˜ê¸°: ì •ê·œí‘œí˜„ì‹(Regex)ì„ ì´ìš© `import re`\n",
    "1. ë¬¸ì¥ ë¶€í˜¸ ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€ \n",
    "2.ëª¨ë“  ë¬¸ìë“¤ì„ ì†Œë¬¸ìë¡œ ë³€í™˜  \n",
    "3.íŠ¹ìˆ˜ë¬¸ìë“¤ì€ ëª¨ë‘ ì œê±° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985f2680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # \n",
    "    return sentence\n",
    "\n",
    "# ì´ ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f6762",
   "metadata": {},
   "source": [
    "ë¬¸ì¥ì„ í† í°í™” í–ˆì„ ë•Œ í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì„ í•™ìŠµ ë°ì´í„°ì—ì„œ ì œì™¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efddd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì—¬ê¸°ì— ì •ì œëœ ë¬¸ì¥ì„ ëª¨ì„ê²ë‹ˆë‹¤\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue  # ë¬¸ì¥ì˜ ëì´ : ì¸ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.\n",
    "    \n",
    "    # ì •ì œë¥¼ í•˜ê³  ë‹´ì•„ì£¼ì„¸ìš”\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "    # í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì€ ì œì™¸í•©ë‹ˆë‹¤.\n",
    "    if len(preprocessed_sentence.split()) > 15: continue\n",
    "        \n",
    "# ì •ì œëœ ê²°ê³¼ë¥¼ 10ê°œë§Œ í™•ì¸í•´ë³´ì£ \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a4473",
   "metadata": {},
   "source": [
    "ë²¡í„°í™”(vectorize)í•˜ê¸°\n",
    "- tf.keras.preprocessing.text.Tokenizer íŒ¨í‚¤ì§€ëŠ” ì •ì œëœ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³ , ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜. \n",
    "- ìˆ«ìë¡œ ë³€í™˜ëœ ë°ì´í„°: í…ì„œ(tensor)\n",
    "\n",
    "####  ë‹¨ì–´ì¥ì˜ í¬ê¸°ëŠ” 12,000 ì´ìƒ ìœ¼ë¡œ ì„¤ì •\n",
    "#### ë¬¸ì¥ ê¸¸ì´ 15ë¡œ ì œí•œ: maxlen ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f4a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1013    3]\n",
      " [  37   15 9049 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f08c46f0700>\n"
     ]
    }
   ],
   "source": [
    "# í† í°í™” í•  ë•Œ í…ì„œí”Œë¡œìš°ì˜ Tokenizerì™€ pad_sequencesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "# ë” ì˜ ì•Œê¸° ìœ„í•´ ì•„ë˜ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 12000ë‹¨ì–´ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” tokenizer(ë‹¨ì–´ì¥)ë¥¼ ë§Œë“¤ê²ë‹ˆë‹¤\n",
    "    # ìš°ë¦¬ëŠ” ì´ë¯¸ ë¬¸ì¥ì„ ì •ì œí–ˆìœ¼ë‹ˆ filtersê°€ í•„ìš”ì—†ì–´ìš”\n",
    "    # 12000ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ëª»í•œ ë‹¨ì–´ëŠ” '<unk>'ë¡œ ë°”ê¿€ê±°ì—ìš”\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±í•©ë‹ˆë‹¤ - ë¬¸ì ë°ì´í„° ì…ë ¥ ë°›ì•„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensor(ì‹œí€€ìŠ¤)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤\n",
    "    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
    "    # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15) # maxlen ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ê¸¸ì´ ì œí•œ \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3543cec",
   "metadata": {},
   "source": [
    "í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ 95% í™•ë¥  ì´ë‚´ë¡œ í¬í•¨í•˜ê²Œ í•˜ëŠ” ì‹ ì‚¬ìš©: mean + 2*std\n",
    "- ì›í•˜ëŠ” ë¬¸ì¥ ê¸¸ì´ê°€ ë‚˜ì˜¤ì§€ ì•Šì•„ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë¡œ í•¨...pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1dbf65c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...    0    0    0]\n",
      " ...\n",
      " [   2  130    5 ...    0    0    0]\n",
      " [ 287   79  162 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fd6be305400>\n"
     ]
    }
   ],
   "source": [
    "# # í† í°í™” í•  ë•Œ í…ì„œí”Œë¡œìš°ì˜ Tokenizerì™€ pad_sequencesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "# # ë” ì˜ ì•Œê¸° ìœ„í•´ ì•„ë˜ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤\n",
    "# # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "# def tokenize(corpus):\n",
    "#     # 12000ë‹¨ì–´ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” tokenizer(ë‹¨ì–´ì¥)ë¥¼ ë§Œë“¤ê²ë‹ˆë‹¤\n",
    "#     # ìš°ë¦¬ëŠ” ì´ë¯¸ ë¬¸ì¥ì„ ì •ì œí–ˆìœ¼ë‹ˆ filtersê°€ í•„ìš”ì—†ì–´ìš”\n",
    "#     # 12000ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ëª»í•œ ë‹¨ì–´ëŠ” '<unk>'ë¡œ ë°”ê¿€ê±°ì—ìš”\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "#         num_words=12000,  \n",
    "#         filters=' ',\n",
    "#         oov_token=\"<unk>\"\n",
    "#     )\n",
    "#     # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±í•©ë‹ˆë‹¤ - ë¬¸ì ë°ì´í„° ì…ë ¥ ë°›ì•„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜\n",
    "#     tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "#     # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensor(ì‹œí€€ìŠ¤)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "#     tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "#     # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤\n",
    "#     # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
    "#     # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "#     tensor = tokenizer.texts_to_sequences(corpus)   # tokenizerëŠ” êµ¬ì¶•í•œ ì‚¬ì „ìœ¼ë¡œë¶€í„° corpusë¥¼ í•´ì„í•´ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "#     total_data_text = list(tensor)\n",
    "#     num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "#     max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "#     maxlen = int(max_tokens)\n",
    "#     # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•œ padding  ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "#     # maxlenì˜ ë””í´íŠ¸ê°’ì€ Noneì…ë‹ˆë‹¤. ì´ ê²½ìš° corpusì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë§ì¶°ì§‘ë‹ˆë‹¤.\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen) # maxlen ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ê¸¸ì´ ì œí•œ \n",
    "    \n",
    "#     print(tensor,tokenizer)\n",
    "#     return tensor, tokenizer\n",
    "\n",
    "# tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd122e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # ê¸¸ì´ê°€ 0ì¸ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.\n",
    "    if sentence[-1] == \":\": continue  # ë¬¸ì¥ì˜ ëì´ : ì¸ ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.\n",
    "\n",
    "    if idx > 9: break   # ì¼ë‹¨ ë¬¸ì¥ 10ê°œë§Œ í™•ì¸í•´ ë³¼ ê²ë‹ˆë‹¤.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d8754",
   "metadata": {},
   "source": [
    "ìƒì„±ëœ í…ì„œ ë°ì´í„° 5í–‰ê¹Œì§€ ì¶œë ¥í•´ë³´ê¸° "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb925e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0\n",
      "     0]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329    3    0    0    0\n",
      "     0]\n",
      " [   2   36    7   37   15  164  282   28  299    4   47    7   43    3\n",
      "     0]\n",
      " [   2   11  354   25   42    3    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    6 3604    4    6 2265    3    0    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "# tokenizerì— êµ¬ì¶•ëœ ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ ì¶œë ¥\n",
    "print(tensor[:5, :]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e3d79",
   "metadata": {},
   "source": [
    "ì •ì œëœ í›„ í…ì„œ, ë¬¸ì¥ í¬ê¸° ì¶œë ¥í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22933b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175749 175749\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor), len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d75f98",
   "metadata": {},
   "source": [
    "ë‹¨ì–´ ì‚¬ì „ì´ ì–´ë–»ê²Œ êµ¬ì¶•ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0f9f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c1629",
   "metadata": {},
   "source": [
    " ìƒì„±ëœ í…ì„œë¥¼ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿìœ¼ë¡œ ë¶„ë¦¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd8d9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ffa1d6",
   "metadata": {},
   "source": [
    "í…ì„œë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì´ìš©í•´ tf.data.Dataset ê°ì²´ ìƒì„±í•˜ê¸°\n",
    "- shuffle(), batch() ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ê´€ë ¨ ê¸°ëŠ¥ í¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46746353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 12000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 7001ê°œ\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# ì¤€ë¹„í•œ ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤\n",
    "# ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”\n",
    "# ìì„¸íˆ ì•Œì•„ë‘˜ìˆ˜ë¡ ë„ì›€ì´ ë§ì´ ë˜ëŠ” ì¤‘ìš”í•œ ë¬¸ì„œì…ë‹ˆë‹¤\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fce242",
   "metadata": {},
   "source": [
    "### Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "- ì´ ë°ì´í„°ì˜ 20% ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b9ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15da5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                    tgt_input, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f097c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586081c5",
   "metadata": {},
   "source": [
    "### Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°\n",
    "\n",
    "- ëª¨ë¸ì˜ Embedding Sizeì™€ Hidden Sizeë¥¼ ì¡°ì ˆí•˜ë©° 10 Epoch ì•ˆì— val_loss ê°’ì„ 2.2 ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•˜ì„¸ìš”! (LossëŠ” ì•„ë˜ ì œì‹œëœ Loss í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56cfc6",
   "metadata": {},
   "source": [
    "modelì— ë°ì´í„°ë¥¼ ì•„ì£¼ ì¡°ê¸ˆ íƒœì›Œ ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af3e0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256  # ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ ë‹¨ì–´ì˜ ì¶”ìƒì ì¸ íŠ¹ì§•ë“¤ì„ ë” ì¡ì•„ë‚¼ ìˆ˜ ìˆì§€ë§Œ, ë°ì´í„° ì–‘ì´ ì¶©ë¶„í•´ì•¼í•¨\n",
    "hidden_size = 1024   # ëª¨ë¸ì— ì–¼ë§ˆë‚˜ ë§ì€ ì¼ê¾¼ì„ ë‘˜ ê²ƒì¸ê°€, ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì£¼ì–´ì ¸ì•¼ ë°°ê°€ ì‚°ìœ¼ë¡œ ê°€ì§€ ì•ŠìŒ\n",
    "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "147cf5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 4.38058487e-05, -1.41151359e-05,  3.74462325e-05, ...,\n",
       "          1.67248116e-04, -7.33670095e-05,  2.41298239e-05],\n",
       "        [ 1.92506734e-04, -4.78628717e-05, -8.75482147e-06, ...,\n",
       "          2.75178929e-04, -2.35268191e-04,  7.68402315e-05],\n",
       "        [ 3.00841435e-04, -1.15203671e-04,  5.58416759e-05, ...,\n",
       "          3.57482641e-04, -4.23312071e-04,  2.58713815e-04],\n",
       "        ...,\n",
       "        [-1.58602124e-04,  1.32800557e-03,  2.81016750e-04, ...,\n",
       "          8.03975854e-04, -3.26197757e-03,  4.60826937e-04],\n",
       "        [-2.51955091e-04,  1.51020614e-03,  2.31753394e-04, ...,\n",
       "          8.88120907e-04, -3.66782234e-03,  4.76644986e-04],\n",
       "        [-3.19709565e-04,  1.68129103e-03,  1.69322011e-04, ...,\n",
       "          9.86161525e-04, -4.04058862e-03,  4.76385118e-04]],\n",
       "\n",
       "       [[-2.39931069e-05,  6.45242180e-05,  2.20165442e-04, ...,\n",
       "          8.66469272e-05,  4.04357794e-04, -1.48011328e-04],\n",
       "        [ 2.57065898e-04,  8.17377222e-05,  5.02168143e-04, ...,\n",
       "          1.95804369e-05,  3.81790422e-04,  5.07867808e-05],\n",
       "        [ 4.69658058e-04,  1.70197571e-04,  5.72694524e-04, ...,\n",
       "         -6.07755919e-06,  4.61100353e-05, -3.60691774e-05],\n",
       "        ...,\n",
       "        [ 1.36265229e-03,  1.03788765e-03, -2.98538525e-05, ...,\n",
       "          1.15131564e-03, -3.23101413e-04, -2.00723251e-03],\n",
       "        [ 1.33644009e-03,  1.03324675e-03,  1.42255347e-04, ...,\n",
       "          1.24877749e-03, -2.34339197e-04, -1.85201736e-03],\n",
       "        [ 1.19692821e-03,  1.22630806e-03,  6.75812189e-05, ...,\n",
       "          1.31989096e-03, -2.68889089e-05, -1.57527672e-03]],\n",
       "\n",
       "       [[ 4.38058487e-05, -1.41151359e-05,  3.74462325e-05, ...,\n",
       "          1.67248116e-04, -7.33670095e-05,  2.41298239e-05],\n",
       "        [ 1.64926212e-04, -3.79555313e-05, -1.54542839e-04, ...,\n",
       "          3.08767078e-04, -1.08759297e-04, -1.84011995e-04],\n",
       "        [ 3.30348237e-04, -1.49961314e-04, -2.68836768e-04, ...,\n",
       "          2.81122251e-04, -2.32382896e-04, -3.51499504e-04],\n",
       "        ...,\n",
       "        [-4.24713595e-04,  4.01146332e-04, -5.63766007e-05, ...,\n",
       "          3.35705408e-04, -1.37110439e-03, -5.47380594e-04],\n",
       "        [-4.26954794e-04,  6.67229062e-04,  3.93198388e-06, ...,\n",
       "          3.91852693e-04, -1.95711013e-03, -3.67306377e-04],\n",
       "        [-4.35115013e-04,  9.22292587e-04,  3.80551537e-05, ...,\n",
       "          4.75426030e-04, -2.52614450e-03, -2.09242178e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.38058487e-05, -1.41151359e-05,  3.74462325e-05, ...,\n",
       "          1.67248116e-04, -7.33670095e-05,  2.41298239e-05],\n",
       "        [ 3.05218331e-04, -1.73579538e-04,  1.86999518e-04, ...,\n",
       "          2.39853136e-04, -1.48957799e-04,  7.61813499e-05],\n",
       "        [ 5.46724768e-04, -3.65278742e-04,  2.74151273e-04, ...,\n",
       "          1.55650225e-04, -3.32203228e-04,  1.93729666e-05],\n",
       "        ...,\n",
       "        [ 1.33474707e-04,  1.30361866e-03,  1.95148561e-04, ...,\n",
       "          6.18808845e-04, -3.36643099e-03,  2.94925587e-04],\n",
       "        [ 8.61898297e-06,  1.51313993e-03,  1.51879212e-04, ...,\n",
       "          7.39405805e-04, -3.81672569e-03,  3.05954163e-04],\n",
       "        [-9.02975735e-05,  1.70134823e-03,  9.86649902e-05, ...,\n",
       "          8.67937342e-04, -4.21368657e-03,  3.08679213e-04]],\n",
       "\n",
       "       [[ 4.38058487e-05, -1.41151359e-05,  3.74462325e-05, ...,\n",
       "          1.67248116e-04, -7.33670095e-05,  2.41298239e-05],\n",
       "        [-1.34812915e-04, -9.07376580e-06,  2.18921283e-04, ...,\n",
       "          1.43181911e-04,  8.83118919e-05,  7.76999514e-05],\n",
       "        [-5.98350889e-05,  8.58471249e-05,  2.78059015e-04, ...,\n",
       "          3.91475478e-04,  2.02990544e-04, -1.41229757e-04],\n",
       "        ...,\n",
       "        [ 2.06500571e-03,  8.55451392e-04,  5.89411589e-04, ...,\n",
       "          2.27642152e-03, -8.26864445e-04, -1.12141459e-03],\n",
       "        [ 1.99096603e-03,  8.36708292e-04,  6.81694655e-04, ...,\n",
       "          2.43886095e-03, -5.71921235e-04, -1.13742938e-03],\n",
       "        [ 1.84968137e-03,  8.18694651e-04,  5.86559530e-04, ...,\n",
       "          2.26785732e-03, -3.92197631e-04, -1.27012213e-03]],\n",
       "\n",
       "       [[ 4.38058487e-05, -1.41151359e-05,  3.74462325e-05, ...,\n",
       "          1.67248116e-04, -7.33670095e-05,  2.41298239e-05],\n",
       "        [-2.53323698e-04, -5.63335343e-05,  1.27020438e-04, ...,\n",
       "          2.43677932e-04,  1.37714276e-04, -6.28152702e-05],\n",
       "        [-4.28599975e-04, -1.69570209e-04,  4.85889526e-04, ...,\n",
       "          1.70233747e-04,  1.12699185e-04, -9.72027119e-05],\n",
       "        ...,\n",
       "        [-9.31040995e-05,  1.04034378e-03, -1.72992717e-04, ...,\n",
       "          1.51972985e-04, -1.92314899e-03,  4.18495125e-04],\n",
       "        [-1.02824211e-04,  1.15599937e-03, -1.28007872e-04, ...,\n",
       "          2.61390203e-04, -2.40252702e-03,  4.58336610e-04],\n",
       "        [-1.34945643e-04,  1.29098725e-03, -1.02124999e-04, ...,\n",
       "          3.83664854e-04, -2.87466985e-03,  4.82438016e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ì—ì„œ ë°ì´í„° í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "# ì§€ê¸ˆì€ ë™ì‘ ì›ë¦¬ì— ë„ˆë¬´ ë¹ ì ¸ë“¤ì§€ ë§ˆì„¸ìš”~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ë´…ë‹ˆë‹¤\n",
    "lyricist(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34c8cf",
   "metadata": {},
   "source": [
    "- ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆ ì¸¡ì • --> ëŒ€ëµ 29million\n",
    "- ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— Output Shapeë¥¼ íŠ¹ì •í•  ìˆ˜ ì—†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16525836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lyricist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63d3fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 124s 178ms/step - loss: 3.6171\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 123s 178ms/step - loss: 3.1302\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 122s 178ms/step - loss: 2.9331\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 123s 179ms/step - loss: 2.7785\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 123s 179ms/step - loss: 2.6446\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 123s 179ms/step - loss: 2.5225\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 123s 179ms/step - loss: 2.4094\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 123s 179ms/step - loss: 2.3033\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 123s 179ms/step - loss: 2.2033\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 123s 179ms/step - loss: 2.1070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07a57283a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "lyricist.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf011c1c",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì´ ìƒì„±í•œ ê°€ì‚¬ í•œ ì¤„ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dbb8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # ë‹¨ì–´ í•˜ë‚˜ì”© ì˜ˆì¸¡í•´ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤\n",
    "    #    1. ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤\n",
    "    #    2. ì˜ˆì¸¡ëœ ê°’ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ word indexë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤\n",
    "    #    3. 2ì—ì„œ ì˜ˆì¸¡ëœ word indexë¥¼ ë¬¸ì¥ ë’¤ì— ë¶™ì…ë‹ˆë‹¤\n",
    "    #    4. ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í–ˆë‹¤ë©´ ë¬¸ì¥ ìƒì„±ì„ ë§ˆì¹©ë‹ˆë‹¤\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = lyricist(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤ \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcd8324f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8627995",
   "metadata": {},
   "source": [
    "## íšŒê³ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78a547",
   "metadata": {},
   "source": [
    "- ì´ë²ˆ í”„ë¡œì íŠ¸ëŠ” ë§Œë“œëŠ” ë‚´ë‚´ ì¡°ê¸ˆ ì¬ë¯¸ìˆì—ˆìŠµë‹ˆë‹¤. ì¸ê°„ì˜ ì˜ì—­ì´ë¼ê³ ë§Œ ìƒê°í–ˆë˜ 'ì°½ì¡°'ë¥¼ AIê°€ í•™ìŠµì„ í†µí•´ ì–´ë–»ê²Œë“  í•´ë‚¸ë‹¤ëŠ”ê²Œ ëŒ€ê²¬í•˜ê¸°ë„ í•˜ê³  ë¿Œë“¯í•˜ë„¤ìš”.\n",
    "\n",
    "- ë‹¤ë§Œ, í† í°í™”í•  ë•Œ ì •ê·œí‘œí˜„ì‹(Regex)ì„ ì´ìš©í•´ì•¼ í–ˆëŠ”ë° ì‚¬ìš©í•˜ëŠ” ê²ƒì— ìµìˆ™í•˜ì§€ê°€ ì•Šì•„ì„œ ì¡°ê¸ˆ ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ìì—°ì–´ì²˜ë¦¬ë¥¼ ê¹Šì´ìˆê²Œ ì—°êµ¬í•˜ë ¤ë©´ ê³µë¶€ê°€ ë” í•„ìš”í•  ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì…ë‹ˆë‹¤. \n",
    "\n",
    "- ë¬¸ì¥ ê¸¸ì´ë¥¼ 15ë¡œ ì œí•œí•˜ëŠ” ê²ƒì— ëŒ€í•´ì„œ ì™œ if len(preprocessed_sentence.split()) > 15: continue:ì„ í–ˆëŠ”ë° ë˜ maxlen=15ë¡œ ì§€ì •í•´ì•¼í•˜ëŠ”ì§€ ì˜ë¬¸ì´ ìƒê²¨ í•œì°¸ì„ ê³ ë¯¼í•˜ê³  ì°¾ì•„ë³´ë‹¤ê°€ ì•Œì•„ë‚´ëŠ” ê³¼ì •ì—ì„œ ë§ì´ ë°°ìš´ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë§Œì•½ ì•ì— ìˆëŠ” ifë¬¸ì„ ì¶”ê°€í•´ì£¼ì§€ ì•Šìœ¼ë©´ í•œ ë¬¸ì¥ì— 15ê°œ ë‹¨ì–´ê°€ ë„˜ëŠ” ê²ƒë“¤ì´ ê±¸ëŸ¬ì§€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ 15ê°œ ë‹¨ì–´ì—ì„œ ì˜ë ¤ì„œ ë‚˜ì˜¬ ê²ƒì´ê¸° ë•Œë¬¸ì— ë‘˜ ë‹¤ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œë˜ì—ˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "`total_data_text = list(tensor)\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)`\n",
    "- max_lenì„ ì´ëŸ°ì‹ìœ¼ë¡œ í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ 95% í™•ë¥  ì´ë‚´ë¡œ í¬í•¨í•˜ê²Œ í•˜ëŠ” ì‹(mean + 2*std)ì„ ì‚¬ìš©í•´ì„œ ì§€ì •í•´ì¤„ ìˆ˜ ìˆëŠ”ë°, ì´ë ‡ê²Œ í†µê³„í•™ì—ì„œ ë°°ì› ë˜ ì§€ì‹ì„ í™œìš©í•´ ì—¬ëŸ¬ ë°©ë²•ë“¤ì„ ìƒê°í•´ë³´ëŠ”ê²Œ ì¬ë¯¸ìˆì—ˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "- ì•ìœ¼ë¡œë„ ì—´ì‹¬íˆ ë°°ìš°ê² ìŠµë‹ˆë‹¤ !!! ğŸ’˜ğŸ’˜ğŸ’˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb892a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ec944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
